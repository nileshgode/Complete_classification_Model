#!/usr/bin/env python
# coding: utf-8

# In[3]:


from mlxtend.plotting import plot_decision_regions
import numpy as np
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
import warnings
warnings.filterwarnings('ignore')
get_ipython().run_line_magic('matplotlib', 'inline')


# In[4]:


os.chdir("C:/Users/nilesh/Desktop/My Python Models/pima-indians-diabetes-database")


# In[5]:


diabetes_data=pd.read_csv('diabetes.csv')


# In[6]:


diabetes_data.head()


# In[7]:


diabetes_data.info(verbose=True)


# # DataFrame.describe() : method 
# generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset‚Äôs distribution, excluding NaN values. This method tells us a lot of things about a dataset. One important thing is that the describe() method deals only with numeric values. It doesn't work with any categorical values. So if there are any categorical values in a column the describe() method will ignore it and display summary for the other columns unless parameter include="all" is passed.
# 
# Now, let's understand the statistics that are generated by the describe() method:
# 
# *count tells us the number of NoN-empty rows in a feature.
# *mean tells us the mean value of that feature.
# *std tells us the Standard Deviation Value of that feature.
# *min tells us the minimum value of that feature.
# *25%, 50%, and 75% are the percentile/quartile of each features. This quartile information helps us to detect Outliers.
# *max tells us the maximum value of that feature.

# In[8]:


diabetes_data.describe()


# In[9]:


diabetes_data.describe().T


# The Question creeping out of this summary
# Can minimum value of below listed columns be zero (0)?
# On these columns, a value of zero does not make sense and thus indicates missing value.
# 
# Following columns or variables have an invalid zero value:
# 
# Glucose,
# BloodPressure,
# SkinThickness,
# Insulin,
# BMI/
# It is better to replace zeros with nan since after that counting them would be easier and zeros need to be replaced with suitable values

# In[10]:


diabetes_data_copy = diabetes_data.copy(deep = True)
diabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)


# In[11]:


print(diabetes_data_copy.isnull().sum())


# # To fill these Nan values the data distribution needs to be understood

# In[12]:


p = diabetes_data.hist(figsize = (20,20))


# # Aiming to impute nan values for the columns in accordance with their distribution

# In[13]:


diabetes_data_copy['Glucose'].fillna(diabetes_data_copy['Glucose'].mean(), inplace = True)
diabetes_data_copy['BloodPressure'].fillna(diabetes_data_copy['BloodPressure'].mean(), inplace = True)
diabetes_data_copy['SkinThickness'].fillna(diabetes_data_copy['SkinThickness'].median(), inplace = True)
diabetes_data_copy['Insulin'].fillna(diabetes_data_copy['Insulin'].median(), inplace = True)
diabetes_data_copy['BMI'].fillna(diabetes_data_copy['BMI'].median(), inplace = True)


# # Plotting after Nan removal

# In[14]:


p = diabetes_data_copy.hist(figsize = (20,20))


# Skewness
# A left-skewed distribution has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. 
# That‚Äôs because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.
# 
# A right-skewed distribution has a long right tail. Right-skewed distributions are also called positive-skew distributions. 
# That‚Äôs because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.

# In[15]:


## observing the shape of the data
diabetes_data.shape


# In[18]:


## null count analysis
import missingno as msno
p=msno.bar(diabetes_data)


# In[19]:


## checking the balance of the data by plotting the count of outcomes by their value
color_wheel = {1: "#0392cf", 
               2: "#7bc043"}
colors = diabetes_data["Outcome"].map(lambda x: color_wheel.get(x + 1))
print(diabetes_data.Outcome.value_counts())
p=diabetes_data.Outcome.value_counts().plot(kind="bar")


# The above graph shows that the data is biased towards datapoints having outcome value as 0 where it means that diabetes was not present actually. 
# The number of non-diabetics is almost twice the number of diabetic patients

# # Scatter matrix of uncleaned data

# In[22]:


from pandas.plotting import scatter_matrix
p=scatter_matrix(diabetes_data,figsize=(25, 25))


# The pairs plot builds on two basic figures, the histogram and the scatter plot. The histogram on the diagonal allows us to see 
# the distribution of a single variable while the scatter plots on the upper and 
# lower triangles show the relationship (or lack thereof) between two variables

# # Pair plot for clean data

# In[23]:


p=sns.pairplot(diabetes_data_copy, hue = 'Outcome')


# Pearson's Correlation Coefficient: helps you find out the relationship between two quantities. 
# It gives you the measure of the strength of association between two variables. 
# The value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 
# 0 means no correlation.
# 
# A heat map is a two-dimensional representation of information with the help of colors. 
# Heat maps can help the user visualize simple or complex information.

# # Heatmap for unclean data

# In[24]:


plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.
p=sns.heatmap(diabetes_data.corr(), annot=True,cmap ='RdYlGn')  
# seaborn has very simple solution for heatmap


# # Heatmap for clean data

# In[25]:


plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.
p=sns.heatmap(diabetes_data_copy.corr(), annot=True,cmap ='RdYlGn') 
# seaborn has very simple solution for heatmap


# Scaling the data
# data Z is rescaled such that Œº = 0 and ùõî = 1

# In[26]:


from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X =  pd.DataFrame(sc_X.fit_transform(diabetes_data_copy.drop(["Outcome"],axis = 1),),
        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])


# In[27]:


X.head()


# In[28]:


y = diabetes_data_copy.Outcome


# About Stratify : Stratify parameter makes a split so that the proportion of values in the sample produced will be the same 
#     as the proportion of values provided to parameter stratify.
# 
# For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, 
# 
# stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.

# In[29]:


#importing train_test_split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1/3,random_state=42, stratify=y)


# In[30]:


from sklearn.neighbors import KNeighborsClassifier

test_scores = []
train_scores = []

for i in range(1,15):

    knn = KNeighborsClassifier(i)
    knn.fit(X_train,y_train)
    
    train_scores.append(knn.score(X_train,y_train))
    test_scores.append(knn.score(X_test,y_test))


# In[31]:


## score that comes from testing on the same datapoints that were used for training
max_train_score = max(train_scores)
train_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]
print('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))


# In[32]:


## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely
max_test_score = max(test_scores)
test_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]
print('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))


# # Result Visualisation

# In[33]:


plt.figure(figsize=(12,5))
p = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')
p = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')


# # The best result is captured at k = 11 hence 11 is used for the final model

# In[34]:


#Setup a knn classifier with k neighbors
knn = KNeighborsClassifier(11)

knn.fit(X_train,y_train)
knn.score(X_test,y_test)


# In[35]:


## trying to plot decision boundary 

value = 20000
width = 20000
plot_decision_regions(X.values, y.values, clf=knn, legend=2, 
                      filler_feature_values={2: value, 3: value, 4: value, 5: value, 6: value, 7: value},
                      filler_feature_ranges={2: width, 3: width, 4: width, 5: width, 6: width, 7: width},
                      X_highlight=X_test.values)

# Adding axes annotations
#plt.xlabel('sepal length [cm]')
#plt.ylabel('petal length [cm]')
plt.title('KNN with Diabetes Data')
plt.show()


# # Model Performance Analysis

# In[36]:


#import confusion_matrix
from sklearn.metrics import confusion_matrix

#let us get the predictions using the classifier we had fit above
y_pred = knn.predict(X_test)
confusion_matrix(y_test,y_pred)
pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)


# In[37]:


y_pred = knn.predict(X_test)
from sklearn import metrics
cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')


# Classification Report
# 
# Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. 
# The question that this metric answer is of all passengers that labeled as survived, how many actually survived? 
# High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good.
# 
# Precision = TP/TP+FP
# 
# Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual 
# class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label? A recall 
#     greater than 0.5 is good.
# 
# Recall = TP/TP+FN
# 
# F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and 
# false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than 
# accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have
# similar cost. If the cost of false positives and false negatives are very different, it‚Äôs better to look at both Precision and
# Recall.
# 
# F1 Score = 2(Recall Precision) / (Recall + Precision)
# 

# In[38]:


#import classification_report
from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))


# # ROC - AUC

# In[39]:


from sklearn.metrics import roc_curve
y_pred_proba = knn.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)


# In[40]:


plt.plot([0,1],[0,1],'k--')
plt.plot(fpr,tpr, label='Knn')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('Knn(n_neighbors=11) ROC curve')
plt.show()


# In[41]:


#Area under ROC curve
from sklearn.metrics import roc_auc_score
roc_auc_score(y_test,y_pred_proba)


# # Hyper Parameter optimization

# Grid search is an approach to hyperparameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.
# 
# Let‚Äôs consider the following example:
# 
# Suppose, a machine learning model X takes hyperparameters a1, a2 and a3. In grid searching, you first define the range of values for each of the hyperparameters a1, a2 and a3. You can think of this as an array of values for each of the hyperparameters. Now the grid search technique will construct many versions of X with all the possible combinations of hyperparameter (a1, a2 and a3) values that you defined in the first place. This range of hyperparameter values is referred to as the grid.
# 
# Suppose, you defined the grid as: a1 = [0,1,2,3,4,5] a2 = [10,20,30,40,5,60] a3 = [105,105,110,115,120,125]
# 
# Note that, the array of values of that you are defining for the hyperparameters has to be legitimate in a sense that you cannot supply Floating type values to the array if the hyperparameter only takes Integer values.
# 
# Now, grid search will begin its process of constructing several versions of X with the grid that you just defined.
# 
# It will start with the combination of [0,10,105], and it will end with [5,60,125]. It will go through all the intermediate combinations between these two which makes grid search computationally very expensive.

# In[42]:


#import GridSearchCV
from sklearn.model_selection import GridSearchCV
#In case of classifier like knn the parameter to be tuned is n_neighbors
param_grid = {'n_neighbors':np.arange(1,50)}
knn = KNeighborsClassifier()
knn_cv= GridSearchCV(knn,param_grid,cv=5)
knn_cv.fit(X,y)

print("Best Score:" + str(knn_cv.best_score_))
print("Best Parameters: " + str(knn_cv.best_params_))


# In[ ]:




